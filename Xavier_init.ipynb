{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to train a mlp to classify the mnist dataset\n",
    "# i will use the torch library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# define the network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# define the training function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "# define the batch size\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# define the training and test datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# define the training and test dataloaders\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302175\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.350215\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.235051\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.377218\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.167971\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.099581\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.183188\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.158153\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.074465\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.057925\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.113625\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.138467\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.020691\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.068514\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.019043\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.136797\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.005300\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.026533\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.025923\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.096547\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.092430\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.017834\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.016083\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.042906\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.062306\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.082894\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.004345\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.111714\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.151430\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.018509\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.063626\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.015722\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.093783\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.016733\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.023885\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.012397\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.085110\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001496\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.123894\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.041766\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.035464\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.170003\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.108618\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.039091\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000694\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.012564\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.002547\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.004754\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.011714\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.006583\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.026245\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.025851\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.035563\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.007067\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.044460\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.068396\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.006538\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.003978\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.060105\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.030252\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016035\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001029\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.035112\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.012013\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.008480\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.057821\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.043713\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.010737\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.083617\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.050774\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.008592\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.018240\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001246\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.037457\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.000915\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.063127\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.035747\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.124205\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.012409\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001817\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.023824\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001882\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.013548\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.050052\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.000223\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.005133\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.010150\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.070836\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.083208\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.003176\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.002641\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.060003\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001351\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000480\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000258\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001436\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.006701\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.018127\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001150\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.050669\n",
      "\n",
      "Test set: Average loss: 0.0788, Accuracy: 9819/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "# define the device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "# test the model\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.273345\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.261519\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.364717\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095610\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.115224\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.159456\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.133708\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.108802\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.057845\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.060870\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.022645\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.080956\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.055946\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.028040\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.039197\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.124865\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.035604\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.079903\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.093539\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.016214\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.150923\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.004539\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.044574\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.045569\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.063785\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.061712\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.053766\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.016607\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.033079\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.056959\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.018762\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.010256\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.115588\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.071645\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.027448\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.010813\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001251\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.109783\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.015989\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.005756\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.086240\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001135\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001823\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.010393\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.008147\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.012631\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.024181\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.019491\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.004447\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003345\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.052302\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.012008\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.036630\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.003032\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.007358\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.006362\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.046810\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.041742\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.056277\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.004519\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005877\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.010131\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.011482\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.020269\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000813\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.026737\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.009413\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.014575\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.016804\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.019907\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.022679\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.000125\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000858\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.097971\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.008043\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.009092\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.019851\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.010885\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.000932\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.016420\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000126\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001624\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000068\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.047494\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003643\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.045978\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.130451\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.003252\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.042236\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000938\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000543\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.004065\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.091509\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000533\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001022\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.008841\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.005578\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.003293\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.040963\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.066423\n",
      "Time taken to train the model with xavier initialization:  47.35460019111633\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306247\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.118112\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.938020\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.845193\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.162930\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.808823\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.326731\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.219963\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.282390\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.164746\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.149604\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.115219\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.054331\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.191623\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.120004\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.035319\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.074276\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.273097\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.075335\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.046123\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.077986\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.038554\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.081593\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.044399\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.065397\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.052627\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.063421\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.026060\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.093072\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.146233\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.048815\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.047554\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.083496\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.024374\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.125217\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.023648\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.034194\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.049996\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.060786\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.011243\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.063852\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.056062\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.189512\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.011596\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.035963\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.042620\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.058551\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.039001\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.024082\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.035446\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.024290\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.035130\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.024840\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.011093\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.016430\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012258\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.008677\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.040200\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.134610\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.043456\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.015919\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003001\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.109392\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001439\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.007147\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.011562\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.135824\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.009242\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.031962\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.003140\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005493\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002643\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.064858\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.004384\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006985\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.004923\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.055913\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.040451\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.002197\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.029399\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002325\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.013017\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.008461\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.013159\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.025734\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.005743\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.006319\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.030424\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.003800\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.002276\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.053219\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.120268\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.025457\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.019853\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.064006\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.008705\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000598\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.027953\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.007928\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.025984\n",
      "Time taken to train the model with zero initialization:  52.85775303840637\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9775/10000 (98%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1113, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Time taken to train the model with xavier initialization:  47.35460019111633\n",
      "Time taken to train the model with zero initialization:  52.85775303840637\n"
     ]
    }
   ],
   "source": [
    "# now i want to compare between intialization methods ie xavier and zero initialization\n",
    "\n",
    "# define the network\n",
    "\n",
    "class MLP_Xavier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Xavier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "# define the network\n",
    "\n",
    "class MLP_Zero(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Zero, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        nn.init.zeros_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# load the models\n",
    "\n",
    "\n",
    "model_xavier = MLP_Xavier()\n",
    "model_zero = MLP_Zero()\n",
    "\n",
    "model_xavier.to(device)\n",
    "model_zero.to(device)\n",
    "\n",
    "# define the optimizers\n",
    "\n",
    "optimizer_xavier = optim.Adam(model_xavier.parameters(), lr=0.001)\n",
    "optimizer_zero = optim.Adam(model_zero.parameters(), lr=0.001)\n",
    "\n",
    "# train the models\n",
    "\n",
    "import stat\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, 11):\n",
    "    train(model_xavier, device, train_loader, optimizer_xavier, epoch)\n",
    "end = time.time()\n",
    "print('Time taken to train the model with xavier initialization: ', end-start)\n",
    "time_xavier = end-start\n",
    "start = time.time()\n",
    "for epoch in range(1, 11):\n",
    "    train(model_zero, device, train_loader, optimizer_zero, epoch)\n",
    "end = time.time()\n",
    "print('Time taken to train the model with zero initialization: ', end-start)\n",
    "time_zero = end-start\n",
    "# test the models\n",
    "\n",
    "model_xavier.eval()\n",
    "model_zero.eval()\n",
    "test_loss_xavier = 0\n",
    "test_loss_zero = 0\n",
    "correct_xavier = 0\n",
    "correct_zero = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output_xavier = model_xavier(data)\n",
    "        output_zero = model_zero(data)\n",
    "        test_loss_xavier += F.nll_loss(output_xavier, target, reduction='sum').item()\n",
    "        test_loss_zero += F.nll_loss(output_zero, target, reduction='sum').item()\n",
    "        pred_xavier = output_xavier.argmax(dim=1, keepdim=True)\n",
    "        pred_zero = output_zero.argmax(dim=1, keepdim=True)\n",
    "        correct_xavier += pred_xavier.eq(target.view_as(pred_xavier)).sum().item()\n",
    "        correct_zero += pred_zero.eq(target.view_as(pred_zero)).sum().item()\n",
    "\n",
    "test_loss_xavier /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss_xavier, correct_xavier, len(test_loader.dataset),\n",
    "    100. * correct_xavier / len(test_loader.dataset)))\n",
    "\n",
    "test_loss_zero /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss_zero, correct_zero, len(test_loader.dataset),\n",
    "    100. * correct_zero / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "print('Time taken to train the model with xavier initialization: ', time_xavier)\n",
    "print('Time taken to train the model with zero initialization: ', time_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
